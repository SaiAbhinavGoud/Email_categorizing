{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPkybCoHyoD+wkNkyEhf5V2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"tuxnEcIeYFft"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from pprint import pprint\n","from time import time\n","\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.pipeline import Pipeline\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem.wordnet import WordNetLemmatizer\n","# upload kaggle token 'kaggle.json'\n","from google.colab import files\n","uploaded=files.upload()"]},{"cell_type":"code","source":["#emails = pd.read_csv('uploaded')\n","import nltk\n","import pandas as pd\n","nltk.download('punkt')\n","nltk.download('stopwords') \n","nltk.download('wordnet')\n","emails = pd.read_csv(next(iter(uploaded)), header=0, index_col=None)\n","\n","em = emails.dropna(axis=0)\n","em.sample(3)\n","\n","em['Category'].value_counts()\n","\n","def pre_process_text(textArray):\n","    #If using stemming...\n","    #stemmer = PorterStemmer()\n","    wnl = WordNetLemmatizer()\n","    processed_text = []\n","    for text in textArray:\n","        words_list = (str(text).lower()).split()\n","        stop_words = set(stopwords.words('english'))\n","        final_words = [wnl.lemmatize(word) for word in words_list if word not in stopwords.words('english')]\n","        #If using stemming...\n","        #final_words = [stemmer.stem(word) for word in words_list if word not in stopwords.words('english')]\n","        final_words_str = str((\" \".join(final_words)))\n","        processed_text.append(final_words_str)\n","    return processed_text\n","\n","\n","em['Subject'] = pre_process_text(em['Subject'])\n","\n","categories = [ 'logistics','tw-commercial group','california','bill williams iii','deal discrepancies','management','calender','esvl','tufco','resumes','e-mail bin','ces','online trading','junk','junk file','ooc','genco','projects','corporate','archives']\n","\n","pipeline = Pipeline([\n","    ('vect', CountVectorizer()),\n","    ('tfidf', TfidfTransformer()),\n","    ('clf', SGDClassifier()),\n","]);\n","\n","# Every additional parameter value here will increase the training time by orders of magnitude.\n","# I'm running on a relatively slow computer, hence reduced the values\n","\n","parameters = {\n","    'vect__max_df': (0.5, 1.0),#0.6, 0.7, 0.8, 0.9, 1.0),\n","    'vect__max_features': (None, 1000, 5000),#2000, 3000, 4000, 5000, 6000, 10000, 20000, 30000, 40000, 50000),\n","    'vect__ngram_range': ((1, 1), (1, 2)),#, (1, 3)),  # unigrams or bigrams or trigrams\n","    'tfidf__use_idf': (True, False),\n","    'tfidf__norm': ('l1', 'l2'),\n","    'clf__alpha': (0.1, 0.01, 0.001),#, 0.0001, 0.00001, 0.000001, 0.0000001),\n","    'clf__penalty': ('l2', 'elasticnet'),\n","    'clf__max_iter': (10, 50),#, 100, 200, 300, 400, 500, 100),\n","}\n","\n","grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, refit=True)\n","\n","print(\"Grid Search started\\n---------------------------------------\")\n","print(\"Pipeline:\", [name for name, _ in pipeline.steps])\n","print(\"Grid Search Parameters:\")\n","print(parameters)\n","t0 = time()\n","grid_search.fit(np.array(em['Subject']), np.array(em['Category']))\n","print(\"done in %0.3fs\\n----------------------------------------------\" % (time() - t0))\n","\n","print(\"Best Score: %0.3f\\n-------------------------------------------\" % grid_search.best_score_)\n","print(\"Best Parameters:\")\n","best_parameters = grid_search.best_estimator_.get_params()\n","for param_name in sorted(parameters.keys()):\n","    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n","\n","param_grid = {\n","                'sgdclassifier__learning_rate':['constant','optimal','invscaling'],\n","                'sgdclassifier__eta0':[0.0,0.01,0.1,0.3,0.5,0.7],\n","                'sgdclassifier__alpha':[0.0001,0.001,0.01,0.1]\n","}\n","\n","pipeline.get_params().keys()\n","\n","import pickle\n","# save the model to disk\n","filename = 'finalized_model.sav'\n","pickle.dump(grid_search, open(filename, 'wb'))\n","\n","test_set = [\n","    'hey there',\n","    'california',\n","    'movie tickets for sale',\n","    'Advice needed for treatment of hair fall',\n","    'Moving out sale',\n","    'RE: Selling Honda City',\n","    'want to grab some offers'\n","]\n","\n","loaded_model.best_estimator_.predict(np.array(test_set))\n","\n","\n","\n"],"metadata":{"id":"1Jic5vjnYPF9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["em['Subject'] = pre_process_text(em['Subject'])\n","\n","categories = [ 'logistics','tw-commercial group','california','bill williams iii','deal discrepancies','management','calender','esvl','tufco','resumes','e-mail bin','ces','online trading','junk','junk file','ooc','genco','projects','corporate','archives']\n","\n","pipeline = Pipeline([\n","    ('vect', CountVectorizer()),\n","    ('tfidf', TfidfTransformer()),\n","    ('clf', SGDClassifier()),\n","]);\n","\n","# Every additional parameter value here will increase the training time by orders of magnitude.\n","# I'm running on a relatively slow computer, hence reduced the values\n","\n","parameters = {\n","    'vect__max_df': (0.5, 1.0),#0.6, 0.7, 0.8, 0.9, 1.0),\n","    'vect__max_features': (None, 1000, 5000),#2000, 3000, 4000, 5000, 6000, 10000, 20000, 30000, 40000, 50000),\n","    'vect__ngram_range': ((1, 1), (1, 2)),#, (1, 3)),  # unigrams or bigrams or trigrams\n","    'tfidf__use_idf': (True, False),\n","    'tfidf__norm': ('l1', 'l2'),\n","    'clf__alpha': (0.1, 0.01, 0.001),#, 0.0001, 0.00001, 0.000001, 0.0000001),\n","    'clf__penalty': ('l2', 'elasticnet'),\n","    'clf__max_iter': (10, 50),#, 100, 200, 300, 400, 500, 100),\n","}"],"metadata":{"id":"vEpCdbUPYTK-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.pipeline import Pipeline\n","grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, refit=True)\n","\n","print(\"Grid Search started\\n---------------------------------------\")\n","print(\"Pipeline:\", [name for name, _ in pipeline.steps])\n","print(\"Grid Search Parameters:\")\n","print(parameters)\n","import time\n","t0 = time.time()\n","grid_search.fit(np.array(em['Subject']), np.array(em['Category']))\n","print(\"done in %0.3fs\\n----------------------------------------------\" % (time() - t0))\n","\n","print(\"Best Score: %0.3f\\n-------------------------------------------\" % grid_search.best_score_)\n","print(\"Best Parameters:\")\n","best_parameters = grid_search.best_estimator_.get_params()\n","for param_name in sorted(parameters.keys()):\n","    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n","\n","param_grid = {\n","                'sgdclassifier__learning_rate':['constant','optimal','invscaling'],\n","                'sgdclassifier__eta0':[0.0,0.01,0.1,0.3,0.5,0.7],\n","                'sgdclassifier__alpha':[0.0001,0.001,0.01,0.1]\n","}\n","\n","pipeline.get_params().keys()"],"metadata":{"id":"xC9jyUe4YWpy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","# save the model to disk\n","filename = 'finalized_model.sav'\n","pickle.dump(grid_search, open(filename, 'wb'))\n","\n","test_set = [\n","    'hey there',\n","    'california',\n","    'movie tickets for sale',\n","    'Advice needed for treatment of hair fall',\n","    'Moving out sale',\n","    'RE: Selling Honda City',\n","    'want to grab some offers'\n","]\n","\n","loaded_model.best_estimator_.predict(np.array(test_set))"],"metadata":{"id":"nvuLlb45YZgG"},"execution_count":null,"outputs":[]}]}